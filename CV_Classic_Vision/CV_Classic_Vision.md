## Early History

1. Alan Turing, 1950
2. Dartmouth Conference, 1956
3. MIT, Minsky, 'spend the summer linking a cam to a computer to describe what it saw', 1966
4. Roerts, Machine Perception of Solids, 1963
5. 3D reconstruction, 1970s/1980s
	1. Extracting edges and inferring the 3D structure
	2. Cylinder or Pictorial
	3. 2.5-D Sketch
	4. Structure from motion
	5. Dense stereo matching
	6. Multi-view
6. Recognitoin? è¯­ä¹‰æ˜¯ classic vision ä¸èƒ½è§£å†³çš„é—®é¢˜
7. Current trend
	1. learning-based method
	2. big data
	3. computation resources

> L2 çš„è‡ªåŠ¨é©¾é©¶è¿˜èƒ½ç”¨ Line fitting åšè½¦é“ä¿æŒï¼Ÿ

# Line fitting

## Images as Functions

1. _Image as a function_ $f$ from $\mathrm{R}^2$ to $\mathrm{R}^M$, $f(x, y)$ gives **intensity**.
   Image contains **discrete** pixels
   å…·ä½“è€Œè¨€, è‰²å½©ç©ºé—´ grayscale: $[0, 255]$, or rgb: Vector3; 
   åˆ†è¾¨ç‡ resolution: $w \times h$ => matrix
   åƒç´ ç©ºé—´ $w \times h \times 3$ => tensor, å·¦ä¸Šè§’ä¸º(0, 0, Color)
2. ç«‹å³å¯æ±‚ _gradient_ $\displaystyle \nabla f = \left[ \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right]$, in practice, use $\displaystyle \frac{\partial f}{\partial x} |_{x = x_0} = \frac{f(x_0+1, y_0) - f(x_0-1, y_0)}{2}$, æŒ‡å®š gradient magnitude $\displaystyle||\nabla f|| = \sqrt{(\frac{\partial f}{\partial x})^2 + (\frac{\partial f}{\partial y})^2}$, point to ä¸edgeå‚ç›´çš„æ–¹å‘
3. _filters_(æ»¤æ³¢): form a new image from original pixels, extract useful information. Modify properties
	1. 1D Filter: çº¿æ€§, å™ªå£°æŠ‘åˆ¶ $h = G(f), h[n] = G(f)[n]$, ä¸€ä¸ªä¾‹å­æ˜¯$\displaystyle h[n] = \frac{\sum_{i=-2}^{i=2} f(x+i)}{5}$,
	2. abstruct: å·ç§¯(convolution, ä¿¡å·ä¸ç³»ç»Ÿ) $\displaystyle h[n] = (f*g)[n] = \sum_{m = -\infty}^{+\infty} f[m]g[n-m]$, ä¸€ä¸ªè§£é‡Šæ˜¯$g$ç¿»è½¬å†ä¸$f$ç§»åŠ¨çª—å£åœ°å¯¹ä½æ±‚ç§¯
   ![](attachment/3fe925f0d263c3b0b95aba2e16424b3c.png)
	3. æœ€é‡è¦çš„å·ç§¯å®šç†: For Fourier Transform $F$, $F(f*g) = F(f)F(g)$, æ—¶åŸŸçš„å·ç§¯ç­‰äºé¢‘è°±çš„ä¹˜ç§¯.
	4. ![](attachment/37a0978aff48c70c9118c0d7da30493a.png)
	   Fourier Transform gives $F(g)[m]$ that mainly concentrates around 0, it's its main feature
	   $g$ act as a ä½é€šæ»¤æ³¢å™¨ low-pass filter
	   ä» Convolution Theorem è§£é‡Šå°±èƒ½å‘ç°å·ç§¯æå–ç‰¹å¾çš„åŸå› : æ»¤å»é«˜é¢‘, ç•™ä¸‹ä½é¢‘, æ»¤å»å™ªå£° =>
	   ![](attachment/7ed0581d307972eba1e2f607a0c52da2.png)
	   filtering $G$ is linear.
	5. if says $loss$ is ç»“æœé¢‘ç‡çš„çº¯æ€§, æˆ‘ä»¬é€šè¿‡å­¦ä¹ æ‰¾åˆ°æœ€å¥½çš„$g$çš„weight
4. _2D discrete filter_: $\displaystyle (f*g)[m, n] = \sum_{k, l} f[k, l]g[m-k, n-l]$, says g as **kernal** or **filter**
	1. ![](attachment/f5a09665e59b24076f5c5826a8f26ef7.png)
	   ä½¿ç”¨å¹³å‡çš„kernalå»é™¤äº†90çš„é«˜é¢‘ä¿¡å·ä½†æ¨¡ç³Šäº†ä¸æ˜¯ç‰¹åˆ«åº”è¯¥æ¨¡ç³Šçš„è¾¹ç•Œ, è¾¹ç•Œä¸€å®šæ˜¯é«˜é¢‘çš„
	2. å†è¿›è¡Œä¸€æ¬¡äºŒå€¼åŒ–, define a threshold é˜ˆå€¼$\tau$, $\displaystyle h[m, n] = \left\{ \begin{aligned} & 1, f[n, m] > \tau \\ & 0, \text{otherwise} \end{aligned} \right.$, non-linear system

## Edge Detection

1. _Define edge_: formulation (ç ”ç©¶èŒƒå¼: Definition first, MathOPè€Œä¸æ˜¯DOP)
   a region that has significant intensity change along one direction **but** low change in its orthogonal direction
2. _Evaluate_ (è¯„ä»·æ ‡å‡† **Evaluation matrix**)
   æ€è€ƒä¸€ä¸‹ä¸å¥½çš„æƒ…å†µ![](attachment/3851b10137202d271dbe6c1148862dcc.png)
   Example: `Low precision`'s $\displaystyle Precision = \frac{1}{3}$, $Recall = 1$.
   æ€è€ƒå¦‚ä½•è¯„å®šå¯¹é½: $\displaystyle localization = \max_{distanceT\to P} < \varepsilon$.
   ç›®æ ‡: good $precision$, good $recall$, low $localization$;
3. _Smooth_: 
	1. Problem. Use derivative. ![](attachment/455d8a228c2634dd87970dec644789b7.png)
	   **But** å¤„å¤„ä¸å¹³æ»‘çš„ noises å¯ä»¥å¾ˆå¥½çš„ hacking gradients => smoothing first.
	   - Gaussian Filter as $g$ to smooth noises: $\displaystyle g = \frac{1}{\sqrt{2\pi \sigma^2}} \exp{- \frac{x^2}{2\sigma^2}}$, the better is $\displaystyle F(g) = \exp{-\frac{\sigma^2 \omega^2}{2}}$.
	   - The bigger $\sigma$ is, the sharper $F(g)$ is. é‚£ä¹ˆå®ƒå°±æ˜¯æ›´å¼ºçš„low-pass filter.
	   - The smaller, the filter weaker.
   ![](attachment/64d2dbc61964f5cbc3599e9180202380.png)
	2. _Optimize_: theorem $\displaystyle \frac{d}{dx} (f*g) = f* \frac{d}{dx} g$, å°†ä¸¤æ­¥åˆä¸ºä¸€æ­¥
	3. _2D Convolution_: Gaussian Filter $\displaystyle g = \frac{1}{2\pi \sigma^2} \exp{-\frac{x^2+y^2}{2 \sigma^2}}$. Use _Optimize_ again.
	4. _Binaryzation_
   ç»“æœ: ![](attachment/fca3f7d07f55ef593eeb0380e36d90c2.png)
4. _NMS_: é—®é¢˜æ˜¯å­˜åœ¨å®½åº¦å¤§äº1çš„æˆåˆ†, æˆ‘ä»¬è¦å»é™¤ Non-Maximal çš„æˆåˆ†, åªç•™ä¸‹æœ€å¥½çš„
	1. Strategy
		1. Get choice  $q$ and its gradient $g(q)$
		2. Find neighbors (Another two choices): $r = q + g(q), p = q - g(q)$.
		   
		   > Problem: $r$ and $p$ (å¤§å¤šæ—¶å€™)éƒ½æ˜¯éæ ¼ç‚¹, æ²¡æœ‰å‡½æ•°å€¼, æ€ä¹ˆåŠ? è¿›è¡ŒåŒçº¿æ€§æ’å€¼ _bilinear interpolation_.
		   
		   ![](attachment/eb800dd2fa4d18d6d1d53860a9764a2d.png)
		   å¾ˆå¥½çš„æ˜¯æ’å€¼ç»“æœä¸æŠ•å½±æ–¹å‘æ— å…³. å¯¹çº¿æ€§æ’å€¼è¿›è¡Œäº†æ€§è´¨å¾ˆä¼˜çš„å»¶æ‹“.
		   
		   > Another approach: ç›´æ¥å¯¹é½åˆ°æ ¼ç‚¹ä¸Š, æ€§èƒ½å¾ˆä¼˜.
		   
		3. Get $g(p)$ and $g(r)$.
		4. $g(p) < g(q) > g(r)$ proves $q$ is a maxium.
	2. ç»“æœ ![](attachment/d787b4cff16ded44e52d65a2a36cbefe.png)
5. _Edge linking_: Hysteresis Thresholding æ»å›é˜ˆå€¼(ä¹±æ)
	1. gradient > maxVal => begin
	2. gradient < minVal => remove
	3. gradient between min and max => connect but no begin

### Canny Edge Detector

Use:
- first derivative
- Gaussian kernal
- optimize signal-to-noise ratio to maxium $precision$ & $recall$

è¶…å‚$\sigma$:
- bigger $\sigma$, bigger $precision$
- smaller $\sigma$, smaller $recall$

## Least Square Method

### Review

1. ç›®æ ‡ï¼šLane Detection
2. å®Œæˆçš„éƒ¨åˆ†ï¼šEdge Detection
3. ç°åœ¨çš„ç›®æ ‡ï¼šä»æ•£ç‚¹åˆ°ç›´çº¿ï¼Œå¾—åˆ°æ›´å¥½çš„ Line Fitting

### SVD

4. _Least Square Method_ï¼šL2 norm - Find $(m, b)$ to minimize $\displaystyle E = \sum_{i=1}^n (y_i - mx_i -b)^2$
	1. let $\displaystyle Y = \left[\begin{matrix}y_{1} \\ \vdots \\ y_{n} \end{matrix}\right]$ , $\displaystyle X = \left[\begin{matrix}x_{1} & 1 \\ \vdots & \vdots \\ x_{n} & 1 \end{matrix}\right]$, $B = \left[\begin{matrix}m\\b\end{matrix}\right]$, $\displaystyle E = ||Y-XB||^2$, finally gives $\displaystyle B = (X^TX)^{-1}X^TY$ï¼Œå¿…è¦æ•°å­¦å‚è§ matrix cookbook
	   
	2. ä¸€ä¸ªå‡¸ä¼˜åŒ–é—®é¢˜ï¼Œä¸”å…·æœ‰å”¯ä¸€å…¨å±€æœ€å°å€¼
	3. Limitation: For **outlier**, it's not robust, ç¦»ç¾¤ç‚¹ä¼šæŠŠçº¿å…¨éƒ¨å¸¦å; fails for vertical line
	4. Improvement: Use $\displaystyle ax+by=d$, to maxmize E, data $\displaystyle A = \begin{bmatrix}x_{1} & y_{1} & 1 \\ \vdots & \vdots & \vdots \\ x_{n} & y_{n} & 1\end{bmatrix}$, $\displaystyle \boldsymbol{h} = \begin{bmatrix}a \\ b \\ d\end{bmatrix}$. æ­¤æ—¶å¾—åˆ°çš„æœ€å°äºŒä¹˜å›å½’æ–¹ç¨‹ $\displaystyle \boldsymbol{Ah} = 0$ï¼Œå¹¶æ»¡è¶³ $\displaystyle E=||\boldsymbol{Ah}||^2$ æœ€å°ï¼›å®ƒæœ‰ä¸€ä¸ªå¹³å‡¡è§£ (0, 0, 0)ï¼›æŒ‡å®š $\displaystyle a^{2}+b^{2}+d^{2}=1$ å¯ä»¥è®©è§£å”¯ä¸€åŒ–ï¼Œå¤±å»å¹³å‡¡è§£
5. _Solve_ï¼šFind $\displaystyle \boldsymbol{h}$, minimize $\displaystyle ||\boldsymbol{Ah}||$, subject to $\displaystyle ||\boldsymbol{h}||=1$
	1. é—®é¢˜è½¬åŒ–ï¼šæ‰¾åˆ°å•ä½çƒé¢ä¸Šçš„ä¸€ä¸ªç‚¹ä½¿å¾—è¿™ä¸ªç‚¹å’ŒAçš„ç‚¹ç§¯æœ€å°
	2. æ–¹é˜µçŸ©é˜µçš„ç‰¹å¾å€¼åˆ†è§£ï¼šå®å¯¹ç§°æ–¹é˜µä¸€å®šå¯ä»¥å¯¹è§’åŒ– => è§£ $\displaystyle Bx=\lambda x$ å¾—åˆ° n ä¸ªç‰¹å¾å€¼ $\displaystyle \lambda$ å’Œ $\displaystyle x_{i}$ => å‡¡æ˜¯å¯ä»¥å¯¹è§’åŒ–çš„çŸ©é˜µ n ä¸ªç‰¹å¾å‘é‡æ„æˆæ­£äº¤åŸº => å–æœ€å°çš„ $\displaystyle \lambda$ æ—¶ $\displaystyle ||\boldsymbol{Bh}||$ æœ€å° (æŠŠå¯¹åº”çš„ç»´åº¦å‹ç¼©åˆ°æœ€å°)
	3. éæ–¹é˜µçŸ©é˜µä¸èƒ½åšç‰¹å¾å€¼åˆ†è§£/å¯¹è§’åŒ– => æå‡ºäº†ä¸€ç§ä»æ–¹é˜µåˆ°éæ–¹é˜µçš„extensionï¼šSingular Value Decomposition (SVD)
6. _Analysis_ï¼š**Robustness** Absolutely still sensitive to outliers; è¿™æ¥è‡ª L2 Norm è¶Šç¦»ç¾¤ gradient å°±è¶Šå¤§çš„ç‰¹æ€§; å¦‚æœç”¨ L1 Normï¼Ÿæ”¶æ•›å°±å¾ˆå›°éš¾

## RANSAC

> Random Sample Consensus: è§£å†³ç¦»ç¾¤ç‚¹çš„é—®é¢˜

1. è‡ªåŠ¨ä¸¢æ‰ç¦»ç¾¤ç‚¹çš„æ–¹æ³•ï¼šå¦‚æœå·²ç»çŸ¥é“äº†ç­”æ¡ˆï¼Œç¦»ç¾¤ç‚¹å°±ä¼šç¦»å®ƒsignificantlyåœ°è¿œï¼›é‚£ä¹ˆå°±éšæœºå–å‡ºä¸€ä¸ªSampleï¼Œæ±‚è¿™ä¸ªSampleæ„æˆçš„hyperplane(å¦‚æœSampleæ•°å¤§äºç»´åº¦å°±è¦æœ€å°äºŒä¹˜æˆ–SVDäº†ï¼Œç­‰äºç»´åº¦å°±å¯ä»¥ç›´æ¥æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„)ï¼Œå†è¡¡é‡Consensusï¼Œè®©æ”¯æŒå®ƒçš„dataseté‡Œå°½å¯èƒ½æ²¡æœ‰outlier
2. _RANSAC Loop_
	1. Initialize: éšæœºå– **2** ä¸ªç‚¹ (æœŸæœ›ä¸åŒ…å«Outlierï¼Œæ‰€ä»¥æ˜¯æœ€å°‘çš„å¯ä»¥å½¢æˆhyperplaneçš„seed group)
	2. Compute: transformation from seed group
	3. Inliers: è®¡æ•°åˆç¾¤ç‚¹çš„æ•°ç›®ï¼Œæ›´æ–°æœ€å¤§Inliers => æ‰¾åˆ°Inlinersæœ€å¤šçš„é‚£ä¸ªSample

> We should implement a parallel version: **Use Tensor, No For Loop**

3. _Hyperparameters_
	1. Hypothesis: å–å‡ºå¤šå°‘ä¸ªç‚¹åšSample
	2. Theoshold: ç®—ä½œInlierçš„é˜ˆå€¼ (ä¾‹å¦‚ $\displaystyle 3\sigma$)
4. _Post-process_ï¼šå¦‚æœå¤šä¸ªseed groupç»™å‡ºäº†ç›¸åŒçš„Inlier numberï¼Ÿæœ´ç´ çš„ä¼˜åŒ– => æŠŠè¿™äº›Inlieré€‰å‡ºæ¥ï¼Œç”¨æœ€å°äºŒä¹˜æˆ–SVDç»™å‡ºæœ€åçš„ç»“æœçš„é‚£ä¸ªæ–¹ç¨‹
5. ä¸ºä»€ä¹ˆè¦é€‰æœ€å°æ•°ç›®çš„hypothesisï¼ŸåŒ…å«nä¸ªç‚¹çš„hypothesiså…¨éƒ½æ­£ç¡®çš„æ¦‚ç‡æ˜¯ $\displaystyle \omega^{n}$ï¼Œæ‰€æœ‰çš„kä¸ªsampleå…¨éƒ¨failçš„æ¦‚ç‡æ˜¯ $\displaystyle (1-\omega^{n})^{k}$ï¼Œæ‰€ä»¥éœ€è¦å°½é‡å‡å°nï¼Œå¢å¤§k
6. _Pros and Cons_
	1. Pros: general, easy
	2. Cons: only moderate percentage of outliers => Voting Strategy, Hough Transform

## Summary

gradient -> edge -> line

> è¿™ä¸ªæ˜¯ module-based systemï¼Œç¼ºç‚¹æ˜¯ä¸€æ­¥å‡ºé”™å°±ä¼šå½±å“å¾ˆå¤šï¼Œéœ€è¦å¾ˆå¼ºçš„robustnessï¼›è‡ªåŠ¨é©¾é©¶ä»å‡ åä¸‡è¡Œçš„ruleåˆ°End2Endçš„systemä¹Ÿæ˜¯è¿™æ ·çš„ï¼›
> Lecturer: æŸå¸å”¯ä¸€çš„ä¸ç«¯åˆ°ç«¯çš„æ˜¯æå–å‡ºçº¢ç»¿ç¯ğŸš¥ç»“æœè¯†åˆ«ï¼Œå†ä¸¢è¿›ç½‘ç»œé‡Œï¼›
> è¿™éƒ¨åˆ†æ€»ä¹‹åªæ˜¯å¯¹ç»å…¸è§†è§‰æ–¹æ³•çš„ç®¡ä¸­çª¥è±¹ï¼›

# Corner Detection

## Harris Detection

1. å›¾ç‰‡çš„ keypoints ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ä¸œè¥¿ï¼Œåº”ç”¨ä¾‹ï¼šä»å…³é”®ç‚¹æ£€æµ‹keypoints localizationï¼Œåˆ°corri-bondingåˆ°image matchingåˆ°ç›¸æœºçš„rotation/transformçš„è¯†åˆ«
2. _Requirements_
	1. Saliency: Interesting points
	2. Repeatability: same result, same object, different images
		1. å¯¹äº®åº¦çš„ä¸å˜æ€§ Illumination invariance
		2. Image scale invariance
		3. Viewpoint invariance
			1. implant rotation: å…³äºç›¸æœºä¸»è½´çš„æ—‹è½¬
			2. affine: äº§ç”Ÿä»¿å°„å˜æ¢ (?)
	3. Accurate localization
	4. Quantity: number sufficient
3. _Corner_
	1. åŸºæœ¬æ»¡è¶³ä¸Šé¢çš„requirements
	2. key property: in the region around the corner, image gradient has two or more dominant directions
	3. Harris Corner: Use a Sliding Window![](attachment/ef55b35b4ef68b8ccf4691829897aedc.png)
	   ç•™æ„æˆ‘ä»¬æ²¡æœ‰å†è¿›è¡Œedge detectionï¼Œè¿™é‡Œè¡¡é‡gradientçš„æ–¹æ³•æ˜¯æ»‘åŠ¨çª—å£å†…çš„intensity
4. _Implement_
	4. Move![](attachment/04e53a6705538e0ebd72f80048e6c212.png)
	5. Square diff![](attachment/7cb099e90d485bb6747c850baab710f2.png)
	6. Window function![](attachment/ec886526c0f17393ada54636efd7805d.png)
	7. Let Square intensity difference $\displaystyle D(x, y)=[I(x+u, y+v)-I(x, y)]^{2}$
	   Window Function $\displaystyle w(x,y)$ which has a param $\displaystyle b$
	   => æˆä¸ºä¸€ä¸ªå·ç§¯æ“ä½œ$$\begin{align}\displaystyle E_{x_{0},y_{0}}(u,v) & =\sum_{x,y}w^{\prime}_{x_{0},y_{0}}(x,y)[I(x+u, y+v)-I(x, y)]^{2} \\ & =\sum_{x,y}w(x_{0}-x,y_{0}-y)D_{u,v}(x,y) \\ & =w*D_{u,v}\end{align}$$
		1. åœ¨ $\displaystyle (x_{0},y_{0})$ å‘¨å›´å…³äº $\displaystyle (u,v)$ åšä¸€é˜¶ Taylor å±•å¼€ï¼Œè¿›è€ŒæŠŠ $\displaystyle D$ å†™æˆäºŒæ¬¡å‹![](attachment/615123bce9a4184c1df4103e107d0334.png)
		   æ³¨æ„æ¯ä¸ªIéƒ½æ˜¯ä¸€ä¸ªå¤§çš„image
		2. Result![](attachment/1d57f8444366eeb0eb19e2c8f016c80e.png)
		3. Analysis![](attachment/ebf1f96567706800c11add25d13ebcd5.png)![](attachment/401d1db044f59a32bc31761588f2f36f.png)
		4. Corner/Edge/Flat? ![](attachment/c57fea32f2f21dbb1820f50b5d7077d0.png)
		5. é—®é¢˜ï¼šå¤šå¤§ç®—å¤§ï¼Ÿå¯èƒ½éœ€è¦å¤šä¸ªè¶…å‚ï¼Œä½†æˆ‘ä»¬åªå…³å¿ƒcornerï¼Œå¯ä»¥å®šä¹‰![](attachment/f371eab6d158f6a37117149b69052d75.png)
		6. let window rotation-invariant => Gaussian window![](attachment/0a0688b7bc2e3014d9a4a0c84784e5fd.png)
5. _Whole Process_ ![](attachment/f0a0aa96afb8fab7ce34222c19f8f0af.png)

> è¿™ä¸€éƒ¨åˆ†æ¨å¯¼è¾ƒå¤šï¼Œé‚ç›´æ¥æ”¾äº†è¯¾å ‚slides

## Properties

> **Definitions**
> If $\displaystyle X\in V$, and $\displaystyle f:V\to V$ is a function, $\displaystyle T:V\to V$ is a transformation operation like translation or rotation, then
> - $\displaystyle f$ to be **equivariant** under $\displaystyle T$ if $\displaystyle T(f(x))=f(T(x))$
> - $\displaystyle f$ to be **invariant** under $\displaystyle T$ if $\displaystyle f(x) = f(T(x))$

1. _Scale_
	1. Harris Detection å¯¹ translation å’Œ rotation (?) éƒ½æ˜¯ equivariant éƒ½æ˜¯ç­‰å˜çš„
	2. To be realistic, æ—‹è½¬åå¯¹ç½‘æ ¼è¿›è¡Œäº†åŒçº¿æ€§å·®å€¼ï¼Œè‡ªç„¶éƒ½ä¸å¯èƒ½æ˜¯rotation-variantçš„
	3. å¯¹ scale ä¸æ˜¯ invariant çš„ (è‡ªç„¶ä¹Ÿæ²¡æœ‰equivariance)ï¼Œä½†é—®é¢˜ä¸å¤§
2. Scale-invariant methods: _Harris Laplacian_ å’Œ _LIFT_